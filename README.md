# Documentation : DÃ©ploiement ETL et Utilisation des Tableaux de Bord BI

Ce document fournit les instructions nÃ©cessaires pour dÃ©ployer le pipeline ETL basÃ© sur Apache Spark et pour utiliser les tableaux de bord Business Intelligence dÃ©veloppÃ©s avec Power BI.

---

## Partie 1 : DÃ©ploiement du Pipeline ETL (Apache Spark & PostgreSQL)

Le pipeline ETL extrait les donnÃ©es de SQL Server (AdventureWorks), les transforme en un modÃ¨le en Ã©toile, puis les charge dans PostgreSQL comme entrepÃ´t de donnÃ©es.

### âœ… PrÃ©requis

* Docker + Docker Compose
* Scripts :

  * ETL compilÃ© (ex: `adventureworks-etl_2.12-1.0.jar`)
  * Pilotes JDBC (SQL Server et PostgreSQL)
  * `docker-compose.yml`
  * Script DDL PostgreSQL (crÃ©ation des tables)

### ğŸ“ Structure des rÃ©pertoires

```
mon-projet-final-data-engeneer/
â”œâ”€â”€ captures/
â”œâ”€â”€ livrable/
â”‚   â””â”€â”€ adventure_warehouse.sql
â”‚   â””â”€â”€ AdventureWorksETL.scala
â”‚   â””â”€â”€ BI_AdventureWorks.pbix
â”‚   â””â”€â”€ Documentation  Instructions pour dÃ©ployer l'ETL et utiliser les tableaux de bord..txt 
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ mssql-jdbc-12.8.1.jre11.jar
â”‚   â””â”€â”€ postgresql-42.7.3.jar
â”‚   â””â”€â”€ postgresql-42.7.3.jar
â”‚   â”œâ”€â”€ src/
â”‚      â””â”€â”€ main/
â”‚       â””â”€â”€ scala
â”‚         â””â”€â”€ AdventureWorksETL.scala
â”‚   â”œâ”€â”€ target/scala-2.12/
â”‚       â””â”€â”€ adventureworksetl_2.12-1.0.jar
```

### ğŸ’¡ Exemple `docker-compose.yml`

```yaml
version: '3.8'
services:
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2019-latest
    environment:
      ACCEPT_EULA: Y
      SA_PASSWORD: "password123?"
    ports:
      - "1433:1433"
    volumes:
      - ./sql_server_init:/docker-entrypoint-initdb.d
    networks:
      - etl_network

  postgresql:
    image: postgres:13
    environment:
      POSTGRES_DB: adventure_warehouse
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: "pass"
    ports:
      - "5432:5432"
    networks:
      - etl_network

  spark-master:
    image: bitnami/spark:3.3.0
    command: /opt/bitnami/spark/bin/spark-shell
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      SPARK_MODE: master
    networks:
      - etl_network

  spark-worker:
    image: bitnami/spark:3.3.0
    command: /opt/bitnami/spark/bin/spark-worker --master spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
    networks:
      - etl_network

  etl-job:
    image: bitnami/spark:3.3.0
    command: >
      /opt/bitnami/spark/bin/spark-submit \
      --class AdventureWorksETL \
      --master spark://spark-master:7077 \
      --jars /app/lib/mssql-jdbc-12.8.1.jre11.jar,/app/lib/postgresql-42.7.3.jar \
      /app/spark_app/target/scala-2.12/adventureworks-etl_2.12-1.0.jar
    volumes:
      - ./lib:/app/lib
      - ./spark_app:/app/spark_app
    depends_on:
      - sqlserver
      - postgresql
      - spark-master
      - spark-worker
    networks:
      - etl_network

networks:
  etl_network:
    driver: bridge
```

### ğŸš€ Lancer le dÃ©ploiement

```bash
docker-compose up --build -d
docker-compose logs etl-job
```

---

## Partie 2 : Utilisation des Tableaux de Bord Power BI

### ğŸ“‚ Connexion PostgreSQL > Power BI

1. Ouvrir Power BI Desktop
2. Accueil > Obtenir les donnÃ©es > PostgreSQL
3. Serveur : `localhost` ou IP du conteneur, DB : `adventure_warehouse`
4. S'authentifier : `postgres / pass`
5. SÃ©lectionner les tables : `DimCustomer`, `DimDate`, `DimGeography`, `DimProduct`, `FactCustomSales`
6. Cliquez sur **Charger**

### ğŸ”„ ModÃ¨le de donnÃ©es

* Vue ModÃ¨le > Assurez-vous que `FactCustomSales` est liÃ©e aux dimensions via les clÃ©s
* CardinalitÃ© : \* vers 1
* Direction : sens unique (de la dimension vers la table de faits)

### ğŸ“Š CrÃ©ation des visualisations

#### 1. âš¡ Ventes par pÃ©riode

* Graphique en courbes ou colonnes
* Axe : `DimDate[FullDate]`
* Valeurs : `FactCustomSales[SalesAmount]`

#### 2. ğŸŒŸ Chiffre dâ€™affaires par catÃ©gorie

* Graphique Ã  secteurs/barres
* Axe/lÃ©gende : `DimProduct[ProductCategory]`
* Valeurs : `SalesAmount`

#### 3. ğŸŒ RÃ©partition gÃ©ographique

* Carte ou graphique Ã  barres
* Axe : `CountryRegion`, `StateProvince`, `City`
* Valeurs : `SalesAmount`

#### 4. ğŸ“ˆ RÃ©partition par segment client

* Graphique en anneau ou secteur
* Axe/lÃ©gende : `DimCustomer[CustomerType]`
* Valeurs : `SalesAmount`

#### 5. ğŸ“Š Taux de croissance des ventes

**Mesures DAX Ã  crÃ©er** :

```DAX
TotalVentes = SUM(FactCustomSales[SalesAmount])

Sales Last Year = CALCULATE([TotalVentes], SAMEPERIODLASTYEAR(DimDate[FullDate]))

TauxCroissance = IF(
    NOT ISBLANK([Sales Last Year]) && [Sales Last Year] <> 0,
    DIVIDE([TotalVentes] - [Sales Last Year], [Sales Last Year]),
    BLANK()
)
```

* Formater `TauxCroissance` en pourcentage
* Visuel : **Carte** (pour KPI global) ou **colonne** (par annÃ©e)

### ğŸ”– Sauvegarde et partage

* Fichier > Enregistrer sous > `BI_AdventureWorks.pbix`
* Partage : Power BI Service (compte Pro ou Premium)

---

> âœ… Cette documentation vous permet de dÃ©ployer un pipeline ETL complet et de construire des visualisations BI puissantes basÃ©es sur vos donnÃ©es AdventureWorks.
